\documentclass{exam}
\usepackage{amsmath, amsfonts}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage[super]{nth}
\usepackage{hyperref}

\DeclareMathOperator*{\argmin}{argmin}

\usepackage[hyperfootnotes=false]{hyperref}

\usepackage[usenames,dvipsnames]{color}
\newcommand{\note}[1]{
	\noindent~\\
	\vspace{0.25cm}
	\fcolorbox{Red}{Orange}{\parbox{0.99\textwidth}{#1\\}}
	%{\parbox{0.99\textwidth}{#1\\}}
	\vspace{0.25cm}
}

\qformat{\thequestion. \textbf{\thequestiontitle}\hfill}
\bonusqformat{\thequestion. \textbf{\thequestiontitle}\hfill}

\pagestyle{headandfoot}

%%%%%% MODIFY FOR EACH SHEET!!!! %%%%%%
\newcommand{\duedate}{25.11.2022 (18:00)}
\newcommand{\due}{{\bf This assignment is due on \duedate.} }
\firstpageheader
{Due: \duedate}
{{\bf\lecture}\\ \assignment{1}}
{\lectors\\ \semester}

\runningheader
{Due: \duedate}
{\assignment{1}}
{\semester}
%%%%%% MODIFY FOR EACH SHEET!!!! %%%%%%

\firstpagefooter
{}
{\thepage}
{}

\runningfooter
{}
{\thepage}
{}

\headrule
\pointsinrightmargin
\bracketedpoints
\marginpointname{pt.}


\begin{document}

\section*{Exercise: Deep Q-Learning}
Find the assignment here: \url{https://classroom.github.com/a/tfnAZTm5}.

\begin{questions}
	\titledquestion{Deep Q Learning}
        This week's exercise aims to develop an intuition about how adding deep learning to value function approximation impacts the learning process. You have been provided with a Q-Learning procedure that uses a shallow neural network as a function approximator for  \href{https://www.gymlibrary.dev/environments/box2d/lunar_lander/}{\texttt{LunarLander-v2}} environment. Your tasks are the following:
        
	\begin{itemize}
		\item Complete the DQN implementation in \emph{deep\_q\_learning.py} by adding a deep network as      a function approximator and a replay buffer to store and sample transitions from.
            \item Vary the network architecture (wider, deeper) and the size of the replay buffer. Record your observations for each choice of architecture and buffer size. Please plot the training curve for your experiments with the \texttt{number of episodes} on the x-axis and the \texttt{mean reward} on the y-axis. The plots should have your choice of architecture as the title and should be stored in a new folder \texttt{plots/}
            \item \textbf{Optional:} The seed can drastically impact your experiment outcome, so it is a common practice in Reinforcement Learning to repeat experiments across multiple seeds and record the training curves as mean values across these seeds with a standard deviation around this value. An example of such a plot can be found \href{https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTl8oaD3BtW2TAjT--6Tn0dzYxJL07ahD1G-z5AEG0Op5ZXZFJ5kr7ZcI4XQtdb7TBTWXc&usqp=CAU}{here}. You could also try to run your experiments across multiple seeds and record your observations in such plots. However, please note that we will not explicitly grade this task. The purpose of this is to show you how experiments are usually conducted in the RL community.
	\end{itemize}

        \textbf{Note:} The tests provided in for this exercise are only an indicator of whether the plots and answers were generated or not, and whether the Q network learned something or not. We will look into the plots and the answers to determine the quality of the submitted solutions.
        
\end{questions}

Please record your answers in \texttt{answers.txt}

\end{document}